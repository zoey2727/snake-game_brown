# snake-game_brown

On 18 July 2025, during my Brown Pre‑College class, I built and trained a reinforcement‑learning version of the classic Snake game. I began by writing a simple game engine and a greedy rule‑based bot to verify that scores updated correctly. From there I set up a virtual Python 3.11 environment, installed PyTorch (with Metal support for my Apple‑Silicon Mac), Gymnasium, and Stable‑Baselines3, and wrapped the game in a Gym‑compatible SnakeEnv. The biggest obstacle was an import disaster of my own making: I had buried several source files inside an extra snake sub‑folder, so Python refused to resolve relative imports and kept throwing “attempted relative import with no known parent package.” Flattening the directory tree, deleting a stray zero‑byte file named simply snake, adding __init__.py, and using the python ‑m snake.<module> launch pattern finally restored order.

With the structure fixed, I trained a small Deep Q‑Network—a two‑layer dense MLP with 64 neurons per layer—on 200 000 interaction steps (roughly 400 episodes). Training took about fifteen minutes on the M‑series GPU and produced a model that reliably scores in the double digits, far outperforming the random baseline that often crashed immediately. The most persistent annoyance was that one forgotten ‑m flag would resurrect the original import error, but once the habit stuck the workflow was smooth: activate the virtual‑env, install packages, run python ‑m snake.train_sb3 to create dqn_snake.zip, and launch python ‑m snake.play_sb3 to watch the agent in action on an ASCII board. Tomorrow I plan to tidy up comments, trim the training run for faster demos, and generate a quick learning‑curve plot to show how the agent improves over time.
